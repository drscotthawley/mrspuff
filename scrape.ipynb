{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape\n",
    "\n",
    "> This is a collection of routines for dataset-building via web scraping, intended to be run on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is modified from Google-scraping code first shared [by akashgshastri on Fast.ai forums](https://forums.fast.ai/t/google-image-scraper/79682/6) which I then updated.\n",
    "\n",
    "*Note: Turns out that Jeremy Howard & Sylvain Gugger had already taught scraping in the [2020 version of the FastAI course](https://github.com/fastai/course2020), and provided some useful routines. So, the code I'd had written previously, I'm going to remove and replace with slighly modified versions of theirs.  (In particular, their DuckDuckGo scraper doesn't require any Selenium ChromeDriver like the Google-scraping code I'd written. Which is great because that was messing up the CI on GitHub anyway.)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import re\n",
    "import requests \n",
    "import json \n",
    "#from fastai.fastcore import *\n",
    "\n",
    "#modified from fastbook utils, https://github.com/fastai/course20/blob/master/fastbook/__init__.py\n",
    "#by Jeremy Howard and Sylvain Gugger.  Just removed the .decode() formatting, and replaced L() with list()\n",
    "def search_images_ddg(key,max_n=200):\n",
    "    \"\"\"By Howard & Gugger: Search for 'key' with DuckDuckGo and return a unique urls of 'max_n' images\n",
    "    (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api)\n",
    "    \"\"\"\n",
    "    url        = 'https://duckduckgo.com/'\n",
    "    params     = {'q':key}\n",
    "    res        = requests.post(url,data=params)\n",
    "    searchObj  = re.search(r'vqd=([\\d-]+)\\&',res.text)\n",
    "    if not searchObj: print('Token Parsing Failed !'); return\n",
    "    requestUrl = url + 'i.js'\n",
    "    headers    = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'}\n",
    "    params     = (('l','us-en'),('o','json'),('q',key),('vqd',searchObj.group(1)),('f',',,,'),('p','1'),('v7exp','a'))\n",
    "    urls       = []\n",
    "    while True:\n",
    "        try:\n",
    "            res  = requests.get(requestUrl,headers=headers,params=params)\n",
    "            data = json.loads(res.text)\n",
    "            for obj in data['results']:\n",
    "                urls.append(obj['image'])\n",
    "                max_n = max_n - 1\n",
    "                if max_n < 1: return list(set(urls))     # dedupe\n",
    "            if 'next' not in data: return list(set(urls))\n",
    "            requestUrl = url + data['next']\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import os, io  \n",
    "from PIL import Image, ImageOps\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def download_and_save(folder_path:str, url:str, verbose:bool=True):\n",
    "    success = False\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "        \n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        #if verbose:  print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")\n",
    "    return success\n",
    "    \n",
    "\n",
    "def search_and_download(search_term:str, target_path:str='./images', num_images:int=10, verbose:bool=True):\n",
    "    \n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    try_urls = search_images_ddg(search_term, max_n=num_images)\n",
    "    print(f\"...got {len(try_urls)} urls for term '{search_term}'\")\n",
    "\n",
    "    count, urls = 0, []      # count success and urls whose images were successfully saved\n",
    "    for url in try_urls:\n",
    "        rc = download_and_save(target_folder, url, verbose=verbose)\n",
    "        if rc:\n",
    "            count += 1\n",
    "            urls.append(url)\n",
    "    \n",
    "    if verbose: print(f\"{search_term}: Expected {num_images}, succeeded at saving {count}.\")\n",
    "    return count, urls \n",
    "\n",
    "\n",
    "# work inprogress\n",
    "class Category():\n",
    "    def __init__(self):\n",
    "        self.images = []\n",
    "        self.urls = []\n",
    "\n",
    "    def __len__(self):\n",
    "        ni, nu = len(self.images), len(self.urls)\n",
    "        #assert ni==nu \n",
    "        return ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "\n",
    "def img_scrape(search_terms:list, target_path:str='./.images', num_images:int=10, verbose:bool=True):\n",
    "    \n",
    "     # clear out directory before use\n",
    "    for category_path in glob.glob(os.path.join(target_path, \"*\")):\n",
    "        shutil.rmtree(category_path)\n",
    "        \n",
    "    dataset = {key: Category() for key in search_terms}\n",
    "    \n",
    "    for term in search_terms:\n",
    "        if verbose: print(f\"Searching on term '{term}'...\")\n",
    "        count, urls = search_and_download(search_term = term, target_path=target_path, num_images=num_images)\n",
    "        dataset[term].urls = urls   # save urls in case we want them later\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Variables to adjust\n",
    "\n",
    "1. set search_term to an array of strings for which you want images\n",
    "2. set num_images to the number of images you want for each class\n",
    "3. set target_path to the path where you want images dataset created.\n",
    "'''\n",
    "search_terms = [\"dog\", \"cat\", \"horse\"]                     \n",
    "#search_terms = [\"les paul guitar\", \"stratocaster guitar\"] # H/T Nathan Sepulveda\n",
    "#search_terms = [\"alligator\", \"crocodile\"]                 # tricky\n",
    "#search_terms = [\"blue sky\", \"stop sign\"]                  # easy: these separate by color!\n",
    "#search_terms = [\"smart person\", \"stupid person\"]          # this is going to be a bad idea! (ethics)\n",
    "\n",
    "num_images = 10\n",
    "target_dir = 'scraped_images'            # where to save to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching on term 'dog'...\n",
      "...got 10 urls for term 'dog'\n",
      "ERROR - Could not download https://thehappypooch.com/wp-content/uploads/2016/02/Labrador-Retriever-Dog.jpg - HTTPSConnectionPool(host='thehappypooch.com', port=443): Max retries exceeded with url: /wp-content/uploads/2016/02/Labrador-Retriever-Dog.jpg (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f2d0e913910>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n",
      "ERROR - Could not save https://thehappypooch.com/wp-content/uploads/2016/02/Labrador-Retriever-Dog.jpg - local variable 'image_content' referenced before assignment\n",
      "dog: Expected 10, succeeded at saving 9.\n",
      "Searching on term 'cat'...\n",
      "...got 10 urls for term 'cat'\n",
      "ERROR - Could not save https://www.askideas.com/media/25/American-Shorthair-Cat-Face-Picture.jpg - cannot identify image file <_io.BytesIO object at 0x7f2d0e915e30>\n",
      "ERROR - Could not save https://www.nationalgeographic.com/content/dam/animals/2019/12/cat-whisperers/cat-whisperers-nationalgeographic_1048225.ngsversion.1576164532899.adapt.1900.1.jpg - cannot identify image file <_io.BytesIO object at 0x7f2d0edecfb0>\n",
      "cat: Expected 10, succeeded at saving 8.\n",
      "Searching on term 'horse'...\n",
      "...got 10 urls for term 'horse'\n",
      "ERROR - Could not save https://hdwallpapers.move.pk/wp-content/uploads/2015/02/beautiful-horse.jpg - cannot identify image file <_io.BytesIO object at 0x7f2d0edecfb0>\n",
      "ERROR - Could not save https://hdwallpapers.move.pk/wp-content/uploads/2015/02/beautiful-horse-style.jpg - cannot identify image file <_io.BytesIO object at 0x7f2d0e8e9dd0>\n",
      "horse: Expected 10, succeeded at saving 8.\n"
     ]
    }
   ],
   "source": [
    "dataset = img_scrape(search_terms, target_path=target_dir, num_images=num_images, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's been saved to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped_images/cat:\n",
      "5db5815f59.jpg\t8d90a88666.jpg\ta95061b4ae.jpg\td4df5a839c.jpg\n",
      "7655f2e055.jpg\ta6a645f3cd.jpg\td439145b0d.jpg\tec6abc5cca.jpg\n",
      "\n",
      "scraped_images/dog:\n",
      "006e6afb04.jpg\t1524e33153.jpg\t6cfef71a27.jpg\tb80c464eef.jpg\tde55a23f16.jpg\n",
      "0a0e166179.jpg\t1c71edd98f.jpg\t88bb07cac8.jpg\tdae216cf27.jpg\n",
      "\n",
      "scraped_images/horse:\n",
      "03a7d5a579.jpg\t3f1f84d4b9.jpg\t8adbd9567c.jpg\te80fc4fbc6.jpg\n",
      "21756204b2.jpg\t6851ee1b64.jpg\tb821ac7db9.jpg\tf17c3d565c.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls {target_dir}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should probably inspect the data to see if it looks good or if we accidentally grabbed images we don't want. \n",
    "\n",
    "### Extra: Interactive Image Browser (Slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 images for dog\n",
      "Loaded 8 images for cat\n",
      "Loaded 8 images for horse\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# Load images from disk\n",
    "for term in search_terms:\n",
    "    dir = term.replace(' ','_')  # spaces to underscores for disk access\n",
    "    dir = f'{target_dir}/{dir}/'\n",
    "    dataset[term].images = [Image.open(item) for i in [glob.glob(f'{dir}*.{ext}') for ext in [\"jpg\",\"gif\",\"png\",\"tga\"]] for item in i]\n",
    "    print(f'Loaded {len(dataset[term].images)} images for {term}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "def browse_images(dataset):\n",
    "    print(\"Select the class from the drop-down, and the image by moving the slider with the mouse or the arrow keys.\")\n",
    "    @interact(term=search_terms)\n",
    "    def _browse_images(term):\n",
    "        n = len(dataset[term])\n",
    "        def view_image(i):\n",
    "            plt.imshow(dataset[term].images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "            plt.show()\n",
    "        interact(view_image, i=(0,n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class from the drop-down, and the image by moving the slider with the mouse or the arrow keys.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2753b2ff7c394e9bac8cc2980fc90f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='term', options=('dog', 'cat', 'horse'), value='dog'), Output()), _…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "browse_images(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a bunch of thumbnails. So, for every file in target_path, load the image, shrink it, and save it to a similar filename in a similar directory structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "#!pip install xattr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import shutil \n",
    "import time \n",
    "from IPython.display import HTML\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "\n",
    "def get_thumb_urls(\n",
    "    images_dir:str=\"scraped_images\",  # directory of full size images, no / on end\n",
    "    size:tuple=(150,150),             # max dims of thumbnail; see PIL Image.thumbnail()\n",
    "    verbose:bool=False                # whether to print status messages or not\n",
    "    ) -> list:\n",
    "    \"\"\"\n",
    "    (Colab only) This will save thumbnails of images and provide 'hosted' urls to them\n",
    "    \"\"\"\n",
    "\n",
    "    if not IN_COLAB:\n",
    "        print(\"Sorry, this only works on Colab\")\n",
    "        return None \n",
    "        \n",
    "    drive.mount('/gdrive')\n",
    "    thumbs_copy_dir = '/gdrive/My Drive/'+ images_dir + \"_thumbs\"\n",
    "    shutil.rmtree(thumbs_copy_dir, ignore_errors=True)      # clear out thumbs dir\n",
    "\n",
    "    # get all the image filenames with full paths\n",
    "    image_paths = [path for path in Path(images_dir).resolve().rglob('*') if path.suffix.lower() in ['.jpg', '.png']]\n",
    "    \n",
    "    # create the thumbnails and save them to Drive \n",
    "    thumb_paths = []\n",
    "    for f in image_paths:\n",
    "        t = Path(thumbs_copy_dir) / f.parent.name / f.name \n",
    "        thumb_paths.append(t)\n",
    "        t.parent.mkdir(parents=True, exist_ok=True)  # create the parent directories before writing files\n",
    "        with Image.open(f) as im: \n",
    "            im.thumbnail(size)\n",
    "            im.save(t)\n",
    "    print(f\"Thumbnails saved to Google Drive in {thumbs_copy_dir}\\nWaiting til URLs are ready.\")\n",
    "\n",
    "    # get thumbnail URLs from Drive (might have to wait a bit for them)\n",
    "    urls = []\n",
    "    for tp in thumb_paths:\n",
    "        count, fid = 0, \"local-225\"  # need a loop in case Drive needs time to generate FileID\n",
    "        while ('local-' in fid) and (count < 100):\n",
    "            fid, count = subprocess.getoutput(f\"xattr -p 'user.drive.id' '{tp}' \"), count+1\n",
    "            if 'local-' in fid: time.sleep(1)\n",
    "        url = f'https://drive.google.com/uc?id={fid}'\n",
    "        urls.append(url)\n",
    "        if verbose: print(f\"url = {url}\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
      "Thumbnails saved to Google Drive in /gdrive/My Drive/scraped_images_thumbs\n",
      "Waiting til URLs are ready.\n",
      "urls =  ['https://drive.google.com/uc?id=1vZpnAhO4EZlSO5nGoizsQalo2R7Kh8_q', 'https://drive.google.com/uc?id=1vPybobt3C4eoVku15SZNmnmRPOVk1USR', 'https://drive.google.com/uc?id=1vO9CQADm4LsMaoSq3WN3JSZFVdNm9RW0', 'https://drive.google.com/uc?id=1vMQnUY3Q6VHiR3ELZwympBKyCILKWdRa', 'https://drive.google.com/uc?id=1vIsxd00eLxjH1rae15LTZWW_uIXPaRMV', 'https://drive.google.com/uc?id=1vHRLPYthJcAJeD68CjT-LmlCezUGR2sM', 'https://drive.google.com/uc?id=1vFdVZyqPJ-oQnyL8ghsi98woFl3LH7X1', 'https://drive.google.com/uc?id=1vFDb2U5uEHcGoRiAtiLn7NmPs1zBWxAy', 'https://drive.google.com/uc?id=1vBZHduZg8wvLN2GDleUrACm64_cjPUR4', 'https://drive.google.com/uc?id=1v66pGlKjmnbCzpEfczkn_pD1uVK897Y2', 'https://drive.google.com/uc?id=1v3_HAlUeAiqutFF9osXpWf7bGCefsWMY', 'https://drive.google.com/uc?id=1uy14rpd-LkpUeuDTGMc46TmSNbRhQr6j', 'https://drive.google.com/uc?id=1upakojCsWWHl5r6_Nf1Oxvvd2Tl-Ng2-', 'https://drive.google.com/uc?id=1ugI1QV5DxLeAfdteYaQX9wghdbLQcTR2', 'https://drive.google.com/uc?id=1ucsxVt296acRKSWmtAXndHVnhosvScak', 'https://drive.google.com/uc?id=1wC71r8Wq3rn9ln-XuUI-j8MTJiWv6VZM', 'https://drive.google.com/uc?id=1w-y7L42teMJifhsJ18oUxpMNanlOJuLz', 'https://drive.google.com/uc?id=1vvNu4x_OQ-Apn1z4FFmDpY5wkPXeEvAv', 'https://drive.google.com/uc?id=1vvAFKCYai3PdzJg9_PcVWNPxN2epElX7', 'https://drive.google.com/uc?id=1vqlYu_l8nuhnRqVbFJOT_fZBS2SmF7Wf', 'https://drive.google.com/uc?id=1vmfGNQUqlA6-2Xn6JnEx29G6cOV-27FA', 'https://drive.google.com/uc?id=1vhsilOD5AXahYEPu1XzX7Q3b5bs4Agel', 'https://drive.google.com/uc?id=1vfHSMiCHTgLw9VjhzBvJfmljaLGrEfUv', 'https://drive.google.com/uc?id=1vf33AW98_yBsHriJJRpx4DMNsc2M7ogK', 'https://drive.google.com/uc?id=1veNTmLSGlv7qncmoGTKO3MTab7-QDWvk']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=https://drive.google.com/uc?id=1vZpnAhO4EZlSO5nGoizsQalo2R7Kh8_q>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = get_thumb_urls(target_dir)\n",
    "print(\"urls = \",urls)\n",
    "url = urls[0] if IN_COLAB else 'https://drive.google.com/uc?id=1owIYMyW7yaYlZ4QcJ8P3iIxkl_mCN-GX'\n",
    "HTML(f\"<img src={url}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
