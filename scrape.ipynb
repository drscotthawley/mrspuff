{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide \n",
    "# Do this on Colab and then restart runtime:\n",
    "#!pip install fastai --upgrade | grep -v 'already satisfied'\n",
    "#!pip install mrspuff --upgrade | grep -v 'already satisfied'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape\n",
    "\n",
    "> This is a collection of routines for dataset-building via web scraping, intended to be run on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is modified from Google-scraping code first shared [by akashgshastri on Fast.ai forums](https://forums.fast.ai/t/google-image-scraper/79682/6) which I then updated.\n",
    "\n",
    "*Note: Turns out that Jeremy Howard & Sylvain Gugger had already taught scraping in the [2020 version of the FastAI course](https://github.com/fastai/course2020), and provided some useful routines. So, the code I'd had written previously, I'm going to remove and replace with slighly modified versions of theirs.  (In particular, their DuckDuckGo scraper doesn't require any Selenium ChromeDriver like the Google-scraping code I'd written. Which is great because that was messing up the CI on GitHub anyway.)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np \n",
    "import re\n",
    "import requests \n",
    "import json \n",
    "import os, io  \n",
    "from PIL import Image, ImageOps\n",
    "import hashlib\n",
    "import shutil\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time \n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from mrspuff.utils import calc_prob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#modified from fastbook utils, https://github.com/fastai/course20/blob/master/fastbook/__init__.py\n",
    "#by Jeremy Howard and Sylvain Gugger.  Just removed the .decode() formatting, and replaced L() with list()\n",
    "def search_images_ddg(key,max_n=200):\n",
    "    \"\"\"By Howard & Gugger: Search for 'key' with DuckDuckGo and return a unique urls of 'max_n' images\n",
    "    (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api)\n",
    "    \"\"\"\n",
    "    url        = 'https://duckduckgo.com/'\n",
    "    params     = {'q':key}\n",
    "    res        = requests.post(url,data=params)\n",
    "    searchObj  = re.search(r'vqd=([\\d-]+)\\&',res.text)\n",
    "    if not searchObj: print('Token Parsing Failed !'); return\n",
    "    requestUrl = url + 'i.js'\n",
    "    headers    = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'}\n",
    "    params     = (('l','us-en'),('o','json'),('q',key),('vqd',searchObj.group(1)),('f',',,,'),('p','1'),('v7exp','a'))\n",
    "    urls       = []\n",
    "    while True:\n",
    "        try:\n",
    "            res  = requests.get(requestUrl,headers=headers,params=params)\n",
    "            data = json.loads(res.text)\n",
    "            for obj in data['results']:\n",
    "                urls.append(obj['image'])\n",
    "                max_n = max_n - 1\n",
    "                if max_n < 1: return list(set(urls))     # dedupe\n",
    "            if 'next' not in data: return list(set(urls))\n",
    "            requestUrl = url + data['next']\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def download_and_save(folder_path:str, url:str, verbose:bool=True):\n",
    "    success = False\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "        \n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        #if verbose:  print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "        success = True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")\n",
    "    return success\n",
    "    \n",
    "\n",
    "def search_and_download(search_term:str, target_path:str='./images', num_images:int=10, verbose:bool=True):\n",
    "    \n",
    "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
    "\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    try_urls = search_images_ddg(search_term, max_n=num_images)\n",
    "    print(f\"...got {len(try_urls)} urls for term '{search_term}'\")\n",
    "\n",
    "    count, urls = 0, []      # count success and urls whose images were successfully saved\n",
    "    for url in try_urls:\n",
    "        rc = download_and_save(target_folder, url, verbose=verbose)\n",
    "        if rc:\n",
    "            count += 1\n",
    "            urls.append(url)\n",
    "    \n",
    "    if verbose: print(f\"{search_term}: Expected {num_images}, succeeded at saving {count}.\")\n",
    "    return count, urls \n",
    "\n",
    "\n",
    "# work inprogress\n",
    "class Category():\n",
    "    def __init__(self):\n",
    "        self.images = []\n",
    "        self.urls = []\n",
    "\n",
    "    def __len__(self):\n",
    "        ni, nu = len(self.images), len(self.urls)\n",
    "        #assert ni==nu \n",
    "        return ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def img_scrape(search_terms:list, target_path:str='./.images', num_images:int=10, verbose:bool=True):\n",
    "    \n",
    "     # clear out directory before use\n",
    "    for category_path in glob.glob(os.path.join(target_path, \"*\")):\n",
    "        shutil.rmtree(category_path)\n",
    "        \n",
    "    dataset = {key: Category() for key in search_terms}\n",
    "    \n",
    "    for term in search_terms:\n",
    "        if verbose: print(f\"Searching on term '{term}'...\")\n",
    "        count, urls = search_and_download(search_term = term, target_path=target_path, num_images=num_images)\n",
    "        dataset[term].urls = urls   # save urls in case we want them later\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "### Variables to adjust\n",
    "\n",
    "1. set search_term to an array of strings for which you want images\n",
    "2. set num_images to the number of images you want for each class\n",
    "3. set target_path to the path where you want images dataset created.\n",
    "'''\n",
    "search_terms = [\"dog\", \"cat\", \"horse\"]                     \n",
    "#search_terms = [\"les paul guitar\", \"stratocaster guitar\"] # H/T Nathan Sepulveda\n",
    "#search_terms = [\"alligator\", \"crocodile\"]                 # tricky\n",
    "#search_terms = [\"blue sky\", \"stop sign\"]                  # easy: these separate by color!\n",
    "#search_terms = [\"smart person\", \"stupid person\"]          # this is going to be a bad idea! (ethics)\n",
    "\n",
    "num_images = 10\n",
    "target_dir = 'scraped_images'            # where to save to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching on term 'dog'...\n",
      "...got 10 urls for term 'dog'\n",
      "ERROR - Could not save http://www.dogbreedslist.info/topics-img/181026-Chinook.jpg - cannot identify image file <_io.BytesIO object at 0x7f751ac97d10>\n",
      "dog: Expected 10, succeeded at saving 9.\n",
      "Searching on term 'cat'...\n",
      "...got 10 urls for term 'cat'\n",
      "ERROR - Could not save https://www.nationalgeographic.com/content/dam/animals/2019/12/cat-whisperers/cat-whisperers-nationalgeographic_1048225.ngsversion.1576164532899.adapt.1900.1.jpg - cannot identify image file <_io.BytesIO object at 0x7f751a3095f0>\n",
      "cat: Expected 10, succeeded at saving 9.\n",
      "Searching on term 'horse'...\n",
      "...got 10 urls for term 'horse'\n",
      "ERROR - Could not save https://hdwallpapers.move.pk/wp-content/uploads/2015/02/bueutiful-horse-running.jpg - cannot identify image file <_io.BytesIO object at 0x7f751ac97c50>\n",
      "ERROR - Could not save https://hdwallpapers.move.pk/wp-content/uploads/2015/02/beautiful-horse-style.jpg - cannot identify image file <_io.BytesIO object at 0x7f751ac97c50>\n",
      "horse: Expected 10, succeeded at saving 8.\n"
     ]
    }
   ],
   "source": [
    "dataset = img_scrape(search_terms, target_path=target_dir, num_images=num_images, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's been saved to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraped_images/cat:\n",
      "1cbc68c4b5.jpg\t5db5815f59.jpg\t7655f2e055.jpg\ta6a645f3cd.jpg\td4df5a839c.jpg\n",
      "566c820ef9.jpg\t617aa15b48.jpg\t8d90a88666.jpg\td439145b0d.jpg\n",
      "\n",
      "scraped_images/dog:\n",
      "045bb79206.jpg\t8115cf9808.jpg\t8cee87adc0.jpg\tac7120d53a.jpg\tdae216cf27.jpg\n",
      "1524e33153.jpg\t88bb07cac8.jpg\t9b59f312a8.jpg\tb2a82a93d6.jpg\n",
      "\n",
      "scraped_images/horse:\n",
      "03a7d5a579.jpg\t21756204b2.jpg\t6435154328.jpg\t8adbd9567c.jpg\n",
      "14a831dca4.jpg\t3f1f84d4b9.jpg\t6851ee1b64.jpg\tb821ac7db9.jpg\n"
     ]
    }
   ],
   "source": [
    "!ls {target_dir}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should probably inspect the data to see if it looks good or if we accidentally grabbed images we don't want. \n",
    "\n",
    "### Extra: Interactive Image Browser (Slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9 images for dog\n",
      "Loaded 9 images for cat\n",
      "Loaded 8 images for horse\n"
     ]
    }
   ],
   "source": [
    "# Load images from disk\n",
    "for term in search_terms:\n",
    "    dir = term.replace(' ','_')  # spaces to underscores for disk access\n",
    "    dir = f'{target_dir}/{dir}/'\n",
    "    dataset[term].images = [Image.open(item) for i in [glob.glob(f'{dir}*.{ext}') for ext in [\"jpg\",\"gif\",\"png\",\"tga\"]] for item in i]\n",
    "    print(f'Loaded {len(dataset[term].images)} images for {term}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def browse_images(dataset):\n",
    "    print(\"Select the class from the drop-down, and the image by moving the slider with the mouse or the arrow keys.\")\n",
    "    @interact(term=search_terms)\n",
    "    def _browse_images(term):\n",
    "        n = len(dataset[term])\n",
    "        def view_image(i):\n",
    "            plt.imshow(dataset[term].images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "            plt.show()\n",
    "        interact(view_image, i=(0,n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the class from the drop-down, and the image by moving the slider with the mouse or the arrow keys.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511bfd84d2464aa0b00f00bc357b2760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='term', options=('dog', 'cat', 'horse'), value='dog'), Output()), _…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "browse_images(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a bunch of thumbnails. So, for every file in target_path, load the image, shrink it, and save it to a similar filename in a similar directory structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "\n",
    "def get_thumb_urls(\n",
    "    images_dir:str=\"scraped_images\",  # directory of full size images, no / on end\n",
    "    size:tuple=(150,150),             # max dims of thumbnail; see PIL Image.thumbnail()\n",
    "    verbose:bool=False                # whether to print status messages or not\n",
    "    ) -> list:\n",
    "    \"\"\"\n",
    "    (Colab only) This will save thumbnails of images and provide 'hosted' urls to them\n",
    "    \"\"\"\n",
    "\n",
    "    if not IN_COLAB:\n",
    "        print(\"Sorry, this only works on Colab\")\n",
    "        return None \n",
    "        \n",
    "    drive.mount('/gdrive')\n",
    "    thumbs_copy_dir = '/gdrive/My Drive/'+ images_dir + \"_thumbs\"\n",
    "    shutil.rmtree(thumbs_copy_dir, ignore_errors=True)      # clear out thumbs dir\n",
    "\n",
    "    # get all the image filenames with full paths\n",
    "    image_paths = [path for path in Path(images_dir).resolve().rglob('*') if path.suffix.lower() in ['.jpg', '.png']]\n",
    "    \n",
    "    # create the thumbnails and save them to Drive \n",
    "    thumb_paths = []\n",
    "    for f in image_paths:\n",
    "        t = Path(thumbs_copy_dir) / f.parent.name / f.name \n",
    "        thumb_paths.append(t)\n",
    "        t.parent.mkdir(parents=True, exist_ok=True)  # create the parent directories before writing files\n",
    "        with Image.open(f) as im: \n",
    "            im.thumbnail(size)\n",
    "            im.save(t)\n",
    "    print(f\"Thumbnails saved to Google Drive in {thumbs_copy_dir}\\nWaiting til URLs are ready.\")\n",
    "\n",
    "    # get thumbnail URLs from Drive (might have to wait a bit for them)\n",
    "    urls = []\n",
    "    for tp in thumb_paths:\n",
    "        count, fid = 0, \"local-225\"  # need a loop in case Drive needs time to generate FileID\n",
    "        while ('local-' in fid) and (count < 100):\n",
    "            fid, count = subprocess.getoutput(f\"xattr -p 'user.drive.id' '{tp}' \"), count+1\n",
    "            if 'local-' in fid: time.sleep(1)\n",
    "        url = f'https://drive.google.com/uc?id={fid}'\n",
    "        urls.append(url)\n",
    "        if verbose: print(f\"url = {url}\")\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "Thumbnails saved to Google Drive in /gdrive/My Drive/scraped_images_thumbs\n",
      "Waiting til URLs are ready.\n",
      "urls =  ['https://drive.google.com/uc?id=10MTXXwwctAglpI3hRg_H1Z6OKdFOo5VD', 'https://drive.google.com/uc?id=10Ga4vfMqMdN0FbZKwiTVveP57uigY7so', 'https://drive.google.com/uc?id=10EPlJZzxM0OmMUv5YVSLpv8dEpxpHqsL', 'https://drive.google.com/uc?id=10DYWaTKsvhg9OIw-XXbafZ99MefjYa0i', 'https://drive.google.com/uc?id=10C4m9ZfTm6ynt_KJPhCJAT-dqLOf4SyO', 'https://drive.google.com/uc?id=10BpS6zi_xU4AMGuP9rjcn_EruYP7TTOb', 'https://drive.google.com/uc?id=107nLiq7s0Dsgx-vUeUu3-x82KTJBodpw', 'https://drive.google.com/uc?id=102_65pl_vrfLEHwYFhVd5DCdgxjMtmIo', 'https://drive.google.com/uc?id=100ioIY6Eh8ienfhMfujOJOMFDps_0P-A', 'https://drive.google.com/uc?id=1-wKbfd6kK6_FZjNnVRRFNi1dT4rkxb_5', 'https://drive.google.com/uc?id=1-rQTBypmgp1DZ3c80FQBY3gAPTqE0fCw', 'https://drive.google.com/uc?id=1-cEEyMtj144-636kAoWgg9d9eXY_GDp9', 'https://drive.google.com/uc?id=1-aHJ2F0akbQS-Ha19rlsjkG2LWsiYFhA', 'https://drive.google.com/uc?id=1-QAgHLOmPtCW5BPm_-3QP1Egk4WjfuRw', 'https://drive.google.com/uc?id=1-Gn6hjqREoQ7HNGLaj-uXsn9M-Lf3Vih', 'https://drive.google.com/uc?id=11-_2IX68wLWvVH0iRhikvPE8vDGgUwZR', 'https://drive.google.com/uc?id=10wEudNsbdipWX_-TNFAsa2TJjKd8upYp', 'https://drive.google.com/uc?id=10tOCKiCDjTEa4IRAFSLZ77ipeqmAy55a', 'https://drive.google.com/uc?id=10ruz7YLYkzZiNgL2gaug5tkHdxmrIqTk', 'https://drive.google.com/uc?id=10j2ByiU8YNw9z1nIayyDHn11lyzzd7vh', 'https://drive.google.com/uc?id=10i1mAzhR5ChlTUVsjmKdxdnHlrOvu2iA', 'https://drive.google.com/uc?id=10bd2uJx3W_xH1xyvlGyYXYHd_xG8ArHU', 'https://drive.google.com/uc?id=10aL5xhrff2CQxCZS5R4f6gqvFcOQOzah', 'https://drive.google.com/uc?id=10YR005XfbD1183-1R9Bq8VD4XnS03pFI', 'https://drive.google.com/uc?id=10SqfdvMMMINWxNphpLDxJMY0jgXPPNCA', 'https://drive.google.com/uc?id=10Mokv1uGIR_XJPa3zpWJIW5t82eAQB7q']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=https://drive.google.com/uc?id=10MTXXwwctAglpI3hRg_H1Z6OKdFOo5VD>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = get_thumb_urls(target_dir)\n",
    "print(\"urls = \",urls)\n",
    "url = urls[0] if IN_COLAB else 'https://drive.google.com/uc?id=1owIYMyW7yaYlZ4QcJ8P3iIxkl_mCN-GX'\n",
    "HTML(f\"<img src={url}>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def exhibit_urls(targ, labels=['cat','dog','horse']):\n",
    "    \"\"\"grabs a set of urls, in order of images that match the labels corresponding to targets\"\"\"\n",
    "    \n",
    "    dim = targ.max()+1\n",
    "    url_store = [[] for t in range(dim)]\n",
    "    for t in range(dim): # for each set of targets, scrape that many urls for the label\n",
    "        label, n = labels[t], np.sum(targ == t )# count how many of each target there are\n",
    "        url_store[t] = search_images_ddg(label)\n",
    "    return [ url_store[targ[t]].pop(0) for t in range(len(targ)) ] # supply a url matching each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, targ = calc_prob(n=400)\n",
    "urls = exhibit_urls(targ, ['cat','dog','horse'])\n",
    "assert len(targ) == len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
